{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761319ec-18e8-46c5-b7d6-bfdabc892e38",
   "metadata": {},
   "source": [
    "## Post-process a finetuned LLM\n",
    "\n",
    "Test and upload a finetuned language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00e6173c-597b-4eb0-8275-bab2c62d4d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U huggingface_hub peft transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d9e47e4-3adf-4aa4-910c-9f75eb213420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e06ad7112f4629abed47b532bba135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25376737-3af9-4382-9240-9e94050c8ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d7ddd-044f-4739-8b01-5a1b187150f5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63449bd1-4434-4f0e-b41c-e3d86815319b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfeb06bd-96d9-4db0-81ee-1053104f915c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 24 20:26:02 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:05:00.0 Off |                  Off |\n",
      "| 35%   56C    P8    20W / 300W |      3MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2057759-7bd5-460a-86fd-c49897fbf79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max memory:  {0: '44GB'}\n"
     ]
    }
   ],
   "source": [
    "free_in_GB = int(torch.cuda.mem_get_info()[0] / 1024**3)\n",
    "max_memory = f\"{free_in_GB-2}GB\"\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "print(\"max memory: \", max_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a3bba-f374-4ed6-acb6-ebedeb6e29be",
   "metadata": {},
   "source": [
    "## Loss curve\n",
    "\n",
    "During training, the model converged nicely as follows:\n",
    "\n",
    "![image](https://raw.githubusercontent.com/daniel-furman/sft-demos/main/assets/jul_24_23_1_13_00_log_loss_curves_llama-2-13b-dolphin.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6076740-c324-49a9-841e-30b4878d208b",
   "metadata": {},
   "source": [
    "## Basic testing\n",
    "\n",
    "With a supervised finetuned (sft) model in hand, we can test it on some basic prompts and then upload it to the Hugging Face hub either as a public or private model repo, depending on the use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b17e2d0-b482-4ebd-a830-325d98318f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570e1e6baad743d1b901354c2f593d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_model_id = \"results/checkpoint-50000\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b93b4abc-50e6-4308-8bfb-dfd164aef43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-39): 40 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "136794be-1cf3-4478-90df-61507179f076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max memory:  {0: '19GB'}\n"
     ]
    }
   ],
   "source": [
    "free_in_GB = int(torch.cuda.mem_get_info()[0] / 1024**3)\n",
    "max_memory = f\"{free_in_GB-2}GB\"\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "print(\"max memory: \", max_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "175e9207-f17b-4ba0-9d3c-cf1ad2f69946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text generation function\n",
    "\n",
    "\n",
    "def llama_generate(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 128,\n",
    "    temperature: int = 1.0,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Initialize the pipeline\n",
    "    Uses Hugging Face GenerationConfig defaults\n",
    "        https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "    Args:\n",
    "        model (transformers.AutoModelForCausalLM): Falcon model for text generation\n",
    "        tokenizer (transformers.AutoTokenizer): Tokenizer for model\n",
    "        prompt (str): Prompt for text generation\n",
    "        max_new_tokens (int, optional): Max new tokens after the prompt to generate. Defaults to 128.\n",
    "        temperature (float, optional): The value used to modulate the next token probabilities.\n",
    "            Defaults to 1.0\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        [prompt],\n",
    "        return_tensors=\"pt\",\n",
    "        return_token_type_ids=False,\n",
    "    ).to(\n",
    "        device\n",
    "    )  # tokenize inputs, load on device\n",
    "\n",
    "    # when running Torch modules in lower precision, it is best practice to use the torch.autocast context manager.\n",
    "    with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        response = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            return_dict_in_generate=True,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    decoded_output = tokenizer.decode(\n",
    "        response[\"sequences\"][0],\n",
    "        skip_special_tokens=True,\n",
    "    )  # grab output in natural language\n",
    "\n",
    "    return decoded_output[len(prompt) :]  # remove prompt from output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd8d36ba-e1a3-444b-b718-af104e130858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Visit the iconic Statue of Liberty and Ellis Island.\n",
      "2. Take a stroll through Central Park and enjoy its many attractions.\n",
      "3. Explore the world-renowned museums, such as the Metropolitan Museum of Art and the Museum of Modern Art.\n",
      "4. Experience the vibrant energy of Times Square and take in the bright lights and billboards.\n",
      "5. Visit the 9/11 Memorial and Museum to pay tribute to those who lost their lives in the attacks.\n",
      "6. Enjoy a Broadway show or a concert at one of the many theaters and venues in the city.\n",
      "7. Take a ride on the Staten Island Ferry for a free view of the Statue of Liberty and the New York City skyline.\n",
      "8. Shop at the famous Fifth Avenue stores and explore the high-end boutiques.\n",
      "9. Indulge in a variety of cuisines at one of the many restaurants in the city.\n",
      "10. Visit the Empire State Building and enjoy the panoramic views of the city from the observation deck.\n",
      "11. Attend a sport\n"
     ]
    }
   ],
   "source": [
    "prompt = \"You are a helpful assistant. Write me a numbered list of things to do in New York City.\\n\"\n",
    "\n",
    "response = llama_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=250,\n",
    "    temperature=0.92,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9753d7f-a9f9-4e96-ac7d-28cd1ef3034b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Understand the situation\n",
      "Daniel needs a haircut and his barber is available on Mondays, Wednesdays, and Fridays.\n",
      "\n",
      "Step 2: Identify the days of the week\n",
      "Sunday is the day Daniel went for a haircut.\n",
      "\n",
      "Step 3: Check if the barber is available on Sundays\n",
      "The barber is not available on Sundays.\n",
      "\n",
      "Step 4: Analyze the situation\n",
      "Since the barber is not available on Sundays, it does not make logical sense for Daniel to go for a haircut on that day.\n",
      "\n",
      "Conclusion: No, this does not make logical sense. Daniel should have gone for a haircut on a day when his barber is available, such as Monday, Wednesday, or Friday.\n",
      "\n",
      "In summary, it does not make logical sense for Daniel to go for a haircut on Sunday because his barber is not available on that day. He should have gone for a haircut on a day when his barber is available, like Monday, Wednesday, or Friday. This would ensure that he can get his haircut when the\n"
     ]
    }
   ],
   "source": [
    "prompt = \"You are a helpful assistant. Daniel is in need of a haircut. His barber works Mondays, Wednesdays, and Fridays. So, Daniel went in for a haircut on Sunday. Does this make logical sense? Let's think this through in a step by step fashion to ensure we have the right answer.\\n\"\n",
    "\n",
    "response = llama_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=250,\n",
    "    temperature=0.92,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3c7f8e8-aa1e-4cec-a6ca-20776da1673d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Friday Dinner Party Invitation\n",
      "\n",
      "Dear Friends,\n",
      "\n",
      "I hope this email finds you well. I'm excited to invite you all to a dinner party on Friday, March 10th, at 7:00 PM. The address is 123 Main Street, Anytown, USA.\n",
      "\n",
      "Please RSVP by Wednesday, March 8th, so I can plan accordingly. I look forward to seeing you all and sharing a delicious meal together!\n",
      "\n",
      "Best,\n",
      "Your Friendly Assistant\n",
      "\n",
      "P.S. If you have any dietary restrictions or allergies, please let me know in your RSVP. Thank you!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"You are a helpful assistant. Write a short email inviting my friends to a dinner party on Friday. Respond succinctly.\\n\"\n",
    "\n",
    "response = llama_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.92,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f802dad-2bcf-4720-a30c-b3e7f89680ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What is a recipe for vegan banana bread?\n",
      "\n",
      "Choose your answer. Are these two questions paraphrases of each other?\n",
      "Choose your answer from:\n",
      "(a). no\n",
      "(b). yes\n",
      "(b). yes\n",
      "\n",
      "Here is a recipe for vegan banana bread:\n",
      "\n",
      "1. Preheat the oven to 350°F (175°C). Grease a 9x5-inch loaf pan with non-stick cooking spray or line it with parchment paper.\n",
      "\n",
      "2. In a large bowl, mash the bananas with a fork until smooth. Add the sugar, oil, and vanilla extract, and mix well.\n",
      "\n",
      "3. In a separate bowl, whisk together the flour, baking powder, baking soda, and salt.\n",
      "\n",
      "4. Add the dry ingredients to the wet ingredients and mix until just combined. Fold in the chocolate chips or walnuts, if using.\n",
      "\n",
      "5. Pour the batter into the prepared loaf pan and spread it evenly. Bake for 50-60 minutes, or until a toothpick inserted into the center comes out clean.\n",
      "\n",
      "6. Allow the bread to cool in the pan for 10 minutes before removing it to a wire rack to cool completely.\n",
      "\n",
      "7. Enjoy your vegan banana bread!\n",
      "\n",
      "This recipe is suitable for vegans as it does not contain any animal products, such as eggs, dairy, or butter. You can adjust the recipe by using different types of flour, sugar, or chocolate chips to suit your taste preferences.\n",
      "\n",
      "(a). no\n",
      "(b). yes\n",
      "\n",
      "These two questions are paraphrases of each other, as they both ask for a recipe for vegan banana bread. The only difference is the wording, but the meaning remains the same.\n",
      "\n",
      "(a). no\n",
      "(b). yes\n",
      "\n",
      "So, the answer is (b). yes. The two questions are paraphrases of each other. They both ask for a recipe for vegan banana bread, but the wording is slightly different. The meaning, however, remains the same.\n",
      "\n",
      "(a). no\n",
      "(b). yes\n",
      "\n",
      "So, the answer is (b). yes. The two questions are paraphrases of each other. They both ask for a recipe for vegan banana bread, but the wording is slightly\n"
     ]
    }
   ],
   "source": [
    "prompt = \"You are a helpful assistant. What is a recipe for vegan banana bread?\\n\"\n",
    "\n",
    "response = llama_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=550,\n",
    "    temperature=0.92,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3ce0ab-d32e-4a7f-91e1-7bd88efc532b",
   "metadata": {},
   "source": [
    "## Inf runtime test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0d1d4a3-e8d3-41f5-92eb-38b1b00eec29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:15<00:00,  3.04s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import time\n",
    "\n",
    "prompt = \"You are a helpful assistant. Write me a long list of things to do in San Francisco:\\n\"\n",
    "\n",
    "runtimes = []\n",
    "for i in tqdm.tqdm(range(25)):\n",
    "    start = time.time()\n",
    "    response = llama_generate(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.92,\n",
    "    )\n",
    "    end = time.time()\n",
    "    runtimes.append(end - start)\n",
    "    assert len(tokenizer.encode(response)) == 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c381a747-4b96-4249-b6ca-1ac1ca7704d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime avg in seconds: 3.0358777046203613\n"
     ]
    }
   ],
   "source": [
    "avg_runtime = torch.mean(torch.tensor(runtimes)).item()\n",
    "print(f\"Runtime avg in seconds: {avg_runtime}\")  # time in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ee6bc1-334f-4bc9-acdf-040ad9a532fd",
   "metadata": {},
   "source": [
    "## Upload model to Hugging Face\n",
    "1. Before running the cells below, create a model on your Hugging Face account. It can be a private or public repo and work with the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1972543-4563-47bf-9950-7dc8551b00db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754ab934a37640a0aeaade7f97645c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/419M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba27834b6e3d46c18eba0af9a9ce5ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/dfurman/llama-2-13b-dolphin-peft/commit/176a21c2dc203b559ea337d918ad83722bd4276b', commit_message='Upload model', commit_description='', oid='176a21c2dc203b559ea337d918ad83722bd4276b', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push to hub\n",
    "model_id_load = \"dfurman/llama-2-13b-dolphin-peft\"\n",
    "\n",
    "# tokenizer\n",
    "tokenizer.push_to_hub(model_id_load, use_auth_token=True)\n",
    "# safetensors\n",
    "model.push_to_hub(model_id_load, use_auth_token=True, safe_serialization=True)\n",
    "# torch tensors\n",
    "model.push_to_hub(model_id_load, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba39e00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "301faebbd5cea7fd4466786a19f1bea9d8baf657aaca95ef39840c46b8697603"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
