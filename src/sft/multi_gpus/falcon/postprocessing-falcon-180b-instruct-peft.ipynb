{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761319ec-18e8-46c5-b7d6-bfdabc892e38",
   "metadata": {},
   "source": [
    "## Post-process a finetuned LLM\n",
    "\n",
    "Test and upload a finetuned language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00e6173c-597b-4eb0-8275-bab2c62d4d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U huggingface_hub peft transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d9e47e4-3adf-4aa4-910c-9f75eb213420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b1d6aa47a2427bb97b61a396cbdc52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25376737-3af9-4382-9240-9e94050c8ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d7ddd-044f-4739-8b01-5a1b187150f5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63449bd1-4434-4f0e-b41c-e3d86815319b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfeb06bd-96d9-4db0-81ee-1053104f915c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 12 15:24:57 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:05:00.0 Off |                  Off |\n",
      "| 30%   35C    P8    22W / 300W |      3MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    On   | 00000000:06:00.0 Off |                  Off |\n",
      "| 30%   32C    P8    24W / 300W |      3MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    On   | 00000000:07:00.0 Off |                  Off |\n",
      "| 30%   35C    P8    22W / 300W |      3MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000    On   | 00000000:08:00.0 Off |                  Off |\n",
      "| 30%   34C    P8    31W / 300W |      3MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2057759-7bd5-460a-86fd-c49897fbf79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max memory:  {0: '44GB', 1: '44GB', 2: '44GB', 3: '44GB'}\n"
     ]
    }
   ],
   "source": [
    "free_in_GB = int(torch.cuda.mem_get_info()[0] / 1024**3)\n",
    "max_memory = f\"{free_in_GB-2}GB\"\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "print(\"max memory: \", max_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a3bba-f374-4ed6-acb6-ebedeb6e29be",
   "metadata": {},
   "source": [
    "## Loss curve\n",
    "\n",
    "During training, the model converged nicely as follows:\n",
    "\n",
    "![image](https://raw.githubusercontent.com/daniel-furman/sft-demos/main/assets/jul_24_23_1_14_00_log_loss_curves_llama-2-70b-dolphin.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6076740-c324-49a9-841e-30b4878d208b",
   "metadata": {},
   "source": [
    "## Basic testing\n",
    "\n",
    "With a supervised finetuned (sft) model in hand, we can test it on some basic prompts and then upload it to the Hugging Face hub either as a public or private model repo, depending on the use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b17e2d0-b482-4ebd-a830-325d98318f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18fa73823b644bc98bae952e47a58259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = \"results/checkpoint-5000\"\n",
    "# peft_model_id = \"dfurman/llama-2-70b-dolphin-peft\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    quantization_config=bnb_config,\n",
    "    use_auth_token=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b93b4abc-50e6-4308-8bfb-dfd164aef43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): FalconForCausalLM(\n",
       "      (transformer): FalconModel(\n",
       "        (word_embeddings): Embedding(65024, 14848)\n",
       "        (h): ModuleList(\n",
       "          (0-79): 80 x FalconDecoderLayer(\n",
       "            (self_attention): FalconAttention(\n",
       "              (maybe_rotary): FalconRotaryEmbedding()\n",
       "              (query_key_value): Linear4bit(\n",
       "                in_features=14848, out_features=15872, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14848, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=15872, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (dense): Linear4bit(\n",
       "                in_features=14848, out_features=14848, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14848, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=14848, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (mlp): FalconMLP(\n",
       "              (dense_h_to_4h): Linear4bit(\n",
       "                in_features=14848, out_features=59392, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14848, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=59392, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (dense_4h_to_h): Linear4bit(\n",
       "                in_features=59392, out_features=14848, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=59392, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=14848, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "            )\n",
       "            (ln_attn): LayerNorm((14848,), eps=1e-05, elementwise_affine=True)\n",
       "            (ln_mlp): LayerNorm((14848,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((14848,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=14848, out_features=65024, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "136794be-1cf3-4478-90df-61507179f076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max memory:  {0: '9GB', 1: '9GB', 2: '9GB', 3: '9GB'}\n"
     ]
    }
   ],
   "source": [
    "free_in_GB = int(torch.cuda.mem_get_info()[0] / 1024**3)\n",
    "max_memory = f\"{free_in_GB-2}GB\"\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "print(\"max memory: \", max_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "175e9207-f17b-4ba0-9d3c-cf1ad2f69946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text generation function\n",
    "\n",
    "\n",
    "def falcon_generate(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 128,\n",
    "    temperature: int = 1.0,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Initialize the pipeline\n",
    "    Uses Hugging Face GenerationConfig defaults\n",
    "        https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "    Args:\n",
    "        model (transformers.AutoModelForCausalLM): Falcon model for text generation\n",
    "        tokenizer (transformers.AutoTokenizer): Tokenizer for model\n",
    "        prompt (str): Prompt for text generation\n",
    "        max_new_tokens (int, optional): Max new tokens after the prompt to generate. Defaults to 128.\n",
    "        temperature (float, optional): The value used to modulate the next token probabilities.\n",
    "            Defaults to 1.0\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        [prompt],\n",
    "        return_tensors=\"pt\",\n",
    "        return_token_type_ids=False,\n",
    "    ).to(\n",
    "        device\n",
    "    )  # tokenize inputs, load on device\n",
    "\n",
    "    # when running Torch modules in lower precision, it is best practice to use the torch.autocast context manager.\n",
    "    with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        response = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            return_dict_in_generate=True,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    decoded_output = tokenizer.decode(\n",
    "        response[\"sequences\"][0],\n",
    "        skip_special_tokens=True,\n",
    "    )  # grab output in natural language\n",
    "\n",
    "    return decoded_output[len(prompt) :]  # remove prompt from output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd8d36ba-e1a3-444b-b718-af104e130858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.92` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Visit the Empire State Building: Take in the breathtaking views of the city from the observation deck on the 86th floor.\n",
      "\n",
      "2. Stroll through Central Park: Enjoy the lush greenery, lakes, and various attractions in this iconic urban park.\n",
      "\n",
      "3. Explore the Metropolitan Museum of Art: Admire the vast collection of art from around the world, spanning thousands of years.\n",
      "\n",
      "4. See a Broadway show: Experience the magic of live theater in one of the world's most famous theater districts.\n",
      "\n",
      "5. Walk across the Brooklyn Bridge: Take in the stunning views of the Manhattan skyline and the East River as you cross this iconic bridge.\n",
      "\n",
      "6. Visit the Statue of Liberty: Take a ferry to Liberty Island and climb to the top of the statue for a unique perspective of the city.\n",
      "\n",
      "7. Visit the 9/11 Memorial and Museum: Pay tribute to the victims of the September 11th attacks and learn about the events that changed the world.\n",
      "\n",
      "8. Visit Times Square: Experience the bright lights and bustling energy of this iconic intersection.\n",
      "\n",
      "9. Visit the High Line: Stroll along this elevated park, built on a former railroad track, and\n"
     ]
    }
   ],
   "source": [
    "prompt = \"You are a helpful assistant. Write me a numbered list of things to do in New York City.\\n\"\n",
    "\n",
    "response = falcon_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=250,\n",
    "    temperature=0.92,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9753d7f-a9f9-4e96-ac7d-28cd1ef3034b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No.\n",
      "\n",
      "The barber does not work on Sundays, so Daniel cannot get a haircut on that day. He should have gone on one of the days the barber works (Mondays, Wednesdays, or Fridays). Therefore, it does not make logical sense for Daniel to go in for a haircut on Sunday. The answer is \"No.\"\n",
      "\n",
      "In summary, the answer is \"No\" because the barber does not work on Sundays, so Daniel cannot get a haircut on\n"
     ]
    }
   ],
   "source": [
    "prompt = \"You are a helpful assistant. Daniel is in need of a haircut. His barber works Mondays, Wednesdays, and Fridays. So, Daniel went in for a haircut on Sunday. Does this make logical sense? Respond only with a yes or no answer in as few words as possible.\\n\"\n",
    "\n",
    "response = falcon_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.92,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3c7f8e8-aa1e-4cec-a6ca-20776da1673d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subject: Dinner Party Invitation - Friday, 7pm\n",
      "\n",
      "Dear friends,\n",
      "\n",
      "I would like to invite you to a dinner party at my place this Friday at 7pm. It would be a great opportunity to catch up and enjoy some delicious food together.\n",
      "\n",
      "Please let me know if you can make it by Wednesday. I look forward to seeing you all!\n",
      "\n",
      "Best,\n",
      "[Your Name]\n",
      "\n",
      "P.S. Please let me know if you have any dietary restrictions.\n",
      "\n",
      "Subject: Dinner Party Invitation - Friday, 7pm\n",
      "\n",
      "Dear friends,\n",
      "\n",
      "I'm hosting a dinner party this Friday at 7pm. Please join me for a fun evening of food and conversation.\n",
      "\n",
      "Let me know if you can make it by Wednesday.\n",
      "\n",
      "Best,\n",
      "[Your Name]\n",
      "\n",
      "P.S. Please let me know if you have any dietary restrictions.\n",
      "\n",
      "Subject: Dinner Party Invitation\n"
     ]
    }
   ],
   "source": [
    "prompt = \"You are a helpful assistant. Write a short email inviting my friends to a dinner party on Friday. Respond succinctly.\\n\"\n",
    "\n",
    "response = falcon_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.92,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f802dad-2bcf-4720-a30c-b3e7f89680ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ingredients:\n",
      "- 3 ripe bananas\n",
      "- 1/3 cup melted coconut oil or vegan butter\n",
      "- 1/4 cup non-dairy milk (almond, soy, or oat milk)\n",
      "- 1 teaspoon vanilla extract\n",
      "- 1/2 cup brown sugar\n",
      "- 1 1/2 cups all-purpose flour\n",
      "- 1 teaspoon baking powder\n",
      "- 1/2 teaspoon baking soda\n",
      "- 1/2 teaspoon salt\n",
      "- 1/2 teaspoon ground cinnamon (optional)\n",
      "- 1/2 cup chopped walnuts or chocolate chips (optional)\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Preheat your oven to 350°F (175°C). Grease a 9x5-inch loaf pan with vegan butter or coconut oil.\n",
      "\n",
      "2. In a large mixing bowl, mash the ripe bananas with a fork until they are smooth.\n",
      "\n",
      "3. Add the melted coconut oil or vegan butter, non-dairy milk, vanilla extract, and brown sugar to the mashed bananas. Mix well until combined.\n",
      "\n",
      "4. In a separate bowl, whisk together the all-purpose flour, baking powder, baking soda, salt, and ground cinnamon (if using).\n",
      "\n",
      "5. Gradually add the dry ingredients to the wet ingredients, mixing until just combined. Do not overmix.\n",
      "\n",
      "6. If you're using walnuts or chocolate chips, gently fold them into the batter.\n",
      "\n",
      "7. Pour the batter into the prepared loaf pan and smooth the top with a spatula.\n",
      "\n",
      "8. Bake for 45-55 minutes, or until a toothpick inserted into the center of the loaf comes out clean.\n",
      "\n",
      "9. Allow the banana bread to cool in the pan for 10 minutes before transferring it to a wire rack to cool completely.\n",
      "\n",
      "10. Slice and serve warm or at room temperature. Enjoy!\n",
      "\n",
      "Note: You can also use whole wheat flour or a gluten-free flour blend in place of all-purpose flour. Adjust the baking time accordingly, as whole wheat flour or gluten-free flour may require a longer baking time.\n",
      "\n",
      "Optional variations:\n",
      "- Add 1/2 cup of chopped nuts (walnuts, pecans, or almonds) or chocolate chips to the batter for extra flavor and texture.\n",
      "- For a more indulgent version, you can add a vegan cream cheese frosting or chocolate gan\n"
     ]
    }
   ],
   "source": [
    "prompt = \"You are a helpful assistant. Tell me a recipe for vegan banana bread.\\n\"\n",
    "\n",
    "response = falcon_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=500,\n",
    "    temperature=0.92,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3ce0ab-d32e-4a7f-91e1-7bd88efc532b",
   "metadata": {},
   "source": [
    "## Inf runtime test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0d1d4a3-e8d3-41f5-92eb-38b1b00eec29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [11:18<00:00, 27.16s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import time\n",
    "\n",
    "prompt = \"You are a helpful assistant. Write me a long list of things to do in San Francisco:\\n\"\n",
    "\n",
    "runtimes = []\n",
    "for i in tqdm.tqdm(range(25)):\n",
    "    start = time.time()\n",
    "    response = falcon_generate(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.92,\n",
    "    )\n",
    "    end = time.time()\n",
    "    runtimes.append(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c381a747-4b96-4249-b6ca-1ac1ca7704d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime avg in seconds: 27.155868530273438\n"
     ]
    }
   ],
   "source": [
    "avg_runtime = torch.mean(torch.tensor(runtimes)).item()\n",
    "print(f\"Runtime avg in seconds: {avg_runtime}\")  # time in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ee6bc1-334f-4bc9-acdf-040ad9a532fd",
   "metadata": {},
   "source": [
    "## Upload model to Hugging Face\n",
    "1. Before running the cells below, create a model on your Hugging Face account. It can be a private or public repo and work with the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1972543-4563-47bf-9950-7dc8551b00db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/utils/hub.py:844: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19836aba6c00489f8aba68824f05bc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/4.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7bb9d50f81b40aa82cd56df29dcb60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/4.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/dfurman/falcon-180b-instruct-peft/commit/d7f7f6db7409e5cf05d7f9fa088a04303b5f8965', commit_message='Upload model', commit_description='', oid='d7f7f6db7409e5cf05d7f9fa088a04303b5f8965', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push to hub\n",
    "model_id_load = \"dfurman/falcon-180b-instruct-peft\"\n",
    "\n",
    "# tokenizer\n",
    "tokenizer.push_to_hub(model_id_load, use_auth_token=True)\n",
    "# safetensors\n",
    "model.push_to_hub(model_id_load, use_auth_token=True, safe_serialization=True)\n",
    "# torch tensors\n",
    "model.push_to_hub(model_id_load, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba39e00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "301faebbd5cea7fd4466786a19f1bea9d8baf657aaca95ef39840c46b8697603"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
